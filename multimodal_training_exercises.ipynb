{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_S1Fku5FKzP"
      },
      "source": [
        "## Setting up environment and importing libraries\n",
        "\n",
        "In this segment, we install the libraries required and set up the environment to train the models. Please choose a GPU runtime in the Google Colab setting. It is also recommended to mount your Google Drive to the notebook so that the static files just need to be downloaded once and can be reused should you need to restart your runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4Wav8jVbrFm"
      },
      "outputs": [],
      "source": [
        "# install required libraries\n",
        "!pip install transformers timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "y4FwkbZBbjz6"
      },
      "outputs": [],
      "source": [
        "# Generic packages\n",
        "import json\n",
        "import os\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "# Deep-Learning packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import AutoModel, AutoTokenizer, get_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Evaluation Packages\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from time import perf_counter\n",
        "\n",
        "# Data Packages\n",
        "from PIL import Image\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXMBm8jE9rlD"
      },
      "source": [
        "Common configurations to be used throughout the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ArUoE-K59rlE"
      },
      "outputs": [],
      "source": [
        "# use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1s_EZjq89rlF"
      },
      "outputs": [],
      "source": [
        "# set random seeds for reprodubility\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(seed_val):\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PAqtMmKz9rlH"
      },
      "outputs": [],
      "source": [
        "seed_val = 0\n",
        "set_seed(seed_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpdcTFbVGLPM"
      },
      "source": [
        "## Data loading and training parameters\n",
        "\n",
        "This segment downloads the data which we are going to use for the tutorial and defines the paths to read data from, as well as training parameters which we are going to use for all three models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "aS7cmvDVGWED"
      },
      "outputs": [],
      "source": [
        "# HOME_FOLDER = '/content/drive/MyDrive/KDD/' # if mounted\n",
        "HOME_FOLDER = '/content/KDD/' # if not mounted\n",
        "WEBVISION_DATA_FOLDER = HOME_FOLDER + 'webvision_data/'\n",
        "IMAGE_FOLDER = WEBVISION_DATA_FOLDER + 'images/'\n",
        "RESULTS_FOLDER = HOME_FOLDER + 'results/'\n",
        "TRAINED_MODELS_FOLDER = HOME_FOLDER + 'trained_models/'\n",
        "os.makedirs(RESULTS_FOLDER, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxEe1QbohJSD"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $WEBVISION_DATA_FOLDER\n",
        "!wget \"https://drive.google.com/uc?id=1r4aTTbLuYgGrgpZLOgUH9sQ33DBsbOFm&export=download\" -O $WEBVISION_DATA_FOLDER/data.zip\n",
        "!unzip $WEBVISION_DATA_FOLDER/data.zip -d $WEBVISION_DATA_FOLDER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mxgXJR36GmLA"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(WEBVISION_DATA_FOLDER + 'train.csv')\n",
        "df_test = pd.read_csv(WEBVISION_DATA_FOLDER + 'test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8bR8_o19rlP"
      },
      "source": [
        "Exceute the cells below to see a random label, text, image triplet from the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IBDGqjLn9rlP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_sample(row_num):\n",
        "    \"\"\"Displays an image at position `row_num` in the WebVision dataset.\"\"\"\n",
        "    sample_row = df_train.iloc[row_num]\n",
        "    print('Index:', row_num)\n",
        "    print('Label:', sample_row['label'])\n",
        "    print('Text:', sample_row['text'])\n",
        "    image_path = IMAGE_FOLDER + sample_row['img_path']\n",
        "    im = Image.open(image_path)\n",
        "    plt.imshow(im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUT59Pcq9rlQ"
      },
      "outputs": [],
      "source": [
        "from random import randint\n",
        "show_sample(randint(0, len(df_train)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahspcDFm9rlR"
      },
      "source": [
        "We create the mapping table to map the string labels to integers to be used for the class labels and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1OrZzFWzG1vS"
      },
      "outputs": [],
      "source": [
        "label_to_id = {lab:i for i, lab in enumerate(df_train['label'].sort_values().unique())}\n",
        "id_to_label = {v:k for k,v in label_to_id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O31tEGXpG5B8"
      },
      "outputs": [],
      "source": [
        "label_to_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rfLhMctDG6xF"
      },
      "outputs": [],
      "source": [
        "num_out_labels = len(label_to_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training parameters that we will use all along this course."
      ],
      "metadata": {
        "id": "gapzdFLnV-bq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hNX_oH_HXQLm"
      },
      "outputs": [],
      "source": [
        "## training parameters to be used for all models ##\n",
        "\n",
        "# Number of passes through the whole training data\n",
        "num_train_epochs = 20\n",
        "\n",
        "# Number of (image, description) samples to be processed at once by the model\n",
        "batch_size = 16\n",
        "\n",
        "# Learning rate used for training all models\n",
        "learning_rate = 1.0e-5\n",
        "\n",
        "# We will also use a learning rate scheduler: it will lowers the learning rate after each model step\n",
        "warmup_steps = 0\n",
        "\n",
        "# Weight decay i.e l2-regularization used for optimizing parameters\n",
        "weight_decay = 0.01\n",
        "\n",
        "# Maximum length in token when encoding the image descriptions\n",
        "max_seq_length = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqdvy6HsH0mu"
      },
      "source": [
        "## BERT\n",
        "The first model which we are going to train is a BERT model which only uses the text from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0aX5qqnIEB1"
      },
      "source": [
        "### Dataset\n",
        "Since we are training a text only model, the dataset which we fit into the model only requires two attributes: **text** and **label**.\n",
        "\n",
        "**Exercise:** Fill the \\_\\_getitem__() function such that it returns the description of an image and its label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOGfUvkeH9Wf"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset used to train a BERT model on the descriptions of the WebVision dataset\"\"\"\n",
        "    def __init__(self, df, label_to_id, text_field=\"text\", label_field=\"label\"):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_to_id = label_to_id\n",
        "        self.text_field = text_field\n",
        "        self.label_field = label_field\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"This default function needs to be defined such that it returns the image description and its label at position index\"\"\"\n",
        "        text = ...\n",
        "        # Do not forget to use the label_to_ids here\n",
        "        label = ...\n",
        "\n",
        "        return text, label\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"This default function should return the length of the dataset\"\"\"\n",
        "\n",
        "        return self.df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrZVgNl7IHqP"
      },
      "source": [
        "### Model\n",
        "The model uses BERT to encode the text, and feeds the encodings (a 768 dimension vector) into a fully connected linear layer with 10 outputs (one for each class label).\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1nlBu9P8saotjNg_nv_tfdnTxpxaFAhqq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQNZ2xACKl3y"
      },
      "outputs": [],
      "source": [
        "## We load the default BERT tokenizer using the transformers library\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what the tokenizer returns: it converts a sentence in a sequence of tokens along with their attention_mask (i.e which tokens can be attended to in self-attention).\n",
        "\n",
        "Note that they are special tokens inserted in each sequence:\n",
        "- **101** is the default mapping for the **[CLS]** token, that is often used to represent full sentences.\n",
        "- **102** is the default mapping for the **[SEP]** token, indicating end of sentence"
      ],
      "metadata": {
        "id": "JUwvbaKvZPiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_example = bert_tokenizer(\"This is an example_text\", truncation=True, max_length=max_seq_length,\n",
        "            return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "print(\"Input IDS: \", tokenized_example.input_ids)\n",
        "print(\"Attention Mask: \", tokenized_example.attention_mask)\n",
        "\n",
        "print(\"Special Token Map: \", {i:j for i,j in zip(bert_tokenizer.all_special_ids, bert_tokenizer.all_special_tokens)})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Yws6MVQZET5",
        "outputId": "8385d644-e064-42f1-ce71-cec653f69389"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDS:  tensor([[ 101, 2023, 2003, 2019, 2742, 1035, 3793,  102]])\n",
            "Attention Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Special Token Map:  {100: '[UNK]', 102: '[SEP]', 0: '[PAD]', 101: '[CLS]', 103: '[MASK]'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ThlwRZ2DH7A_"
      },
      "outputs": [],
      "source": [
        "class VLBertModel(nn.Module):\n",
        "    \"\"\"PyTorch model that will be used to train the BERT model on the WebVision dataset.\"\"\"\n",
        "    def __init__(self, num_labels, text_pretrained='bert-base-uncased'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        ## The text_encoder attribute is the default pretrained BERT model\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_pretrained)\n",
        "        ## BERT maps a sequence of text to embeddings. We need to add a projection layer to map the embeddings to the number of classes\n",
        "\n",
        "        # Exercise: Define the classifier attribute to be a linear layer that maps embeddings of BERT size to the number of labels.\n",
        "        #   linear layer: nn.Linear\n",
        "        #   BERT Embeddings size: self.text_encoder.config.hidden_size\n",
        "        self.classifier = ...\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        \"\"\"The forward function takes data input and should return label likelihood, named `logits`.\"\"\"\n",
        "        # Excerise: fill the blanks with the right inputs\n",
        "        output = self.text_encoder(input_ids=..., attention_mask=..., return_dict=True)\n",
        "        # Here, we take the last hidden state of the output, which is defined by the embeddings returned by the last BERT layer\n",
        "        logits = self.classifier(output.last_hidden_state[:, ..., :]) # CLS embedding\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qDOw8G2EI3Bv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "de2d96325bc947e9a3fc1f587bfe3eb4",
            "d60acccde21642129a3f1e9fe66c8784",
            "7f63c2f548f0478d88d6c2bffcffa1a2",
            "3f3562fa5c0a4e0ea4f8be5b49ed8e5d",
            "f811840e755e424dba04de4a081414a8",
            "4f902f3c8ed74a868be7f50b5b25733e",
            "05166af3c5af409d84a59792991eb8fc",
            "e6edb9af6e494b3fae39b257c53d301b",
            "e0f41b6fbea640ebb85c9709c39a90e5",
            "777fd002e1dc497493cbcabacb22d979",
            "53d331f802a1429ba2b7a0ae927ee061"
          ]
        },
        "outputId": "0449bd89-c120-46ee-8dd0-b49bbb364a6b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de2d96325bc947e9a3fc1f587bfe3eb4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# create the model\n",
        "bert_model = VLBertModel(num_labels=num_out_labels, text_pretrained='bert-base-uncased')\n",
        "# Map to GPU\n",
        "bert_model = bert_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzia70xiLLFi"
      },
      "source": [
        "### Training\n",
        "Load the data using the text dataset, feed it into a data loader for random sampling, and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWcI7V-hKKLp"
      },
      "outputs": [],
      "source": [
        "set_seed(seed_val)\n",
        "\n",
        "# Define the dataset and the DataLoader\n",
        "train_dataset = TextDataset(df=df_train, label_to_id=label_to_id, text_field='text', label_field='label')\n",
        "# The DataLoader will take care of separating the dataset in batches of size `batch_size`\n",
        "# The RandomSampler will take care of randomly splitting the data\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    sampler=train_sampler)\n",
        "\n",
        "# Total number of steps\n",
        "t_total = len(train_dataloader) * num_train_epochs\n",
        "\n",
        "# We use the AdamW optimizer here\n",
        "optimizer = AdamW(bert_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "# We also use a cosine scheduler: it will take care of lower the learning rate after each optimizer step.\n",
        "# Learning rate scheduling is often used to reduce overfitting on data\n",
        "scheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "# Here, we will use a cross entropy loss because it's a multiclass problem\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Put the model in train mode: (enables special features such as dropout, batch normalization...)\n",
        "bert_model.train()\n",
        "\n",
        "\n",
        "start = perf_counter()\n",
        "\n",
        "# Run over all epochs\n",
        "for epoch_num in trange(num_train_epochs, desc='Epochs'):\n",
        "    epoch_total_loss = 0\n",
        "    # For each epoch, run over all batches of data\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc='Batch'):\n",
        "        b_text, b_labels = batch\n",
        "        # Tokenize the image description\n",
        "        b_inputs = bert_tokenizer(\n",
        "            list(b_text), truncation=True, max_length=max_seq_length,\n",
        "            return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "        # Put input data to GPU\n",
        "        b_labels = b_labels.to(device)\n",
        "        b_inputs = b_inputs.to(device)\n",
        "\n",
        "        # PYTORCH TRAINING LOOP\n",
        "\n",
        "        # Exercise: fill the pytorch training loop\n",
        "\n",
        "        ## Step 1: gradient should be set to 0\n",
        "        ...\n",
        "        ## Step 2: pass the inputs to the model and get logits\n",
        "        b_logits = ...\n",
        "        ## Step 3: Calculate loss given the logits and the labels\n",
        "        loss = ...\n",
        "\n",
        "        ## accumulates all losses\n",
        "        epoch_total_loss += loss.item()\n",
        "\n",
        "        ## Perform a backward pass to calculate the gradients\n",
        "        ...\n",
        "\n",
        "        ## Perform the optimizer step: backpropagation on model parameters\n",
        "        ...\n",
        "        ## Perform the scheduelr step: lowers the learning rate according to the schedule\n",
        "        ...\n",
        "\n",
        "    avg_loss = epoch_total_loss/len(train_dataloader)\n",
        "\n",
        "\n",
        "    print('epoch =', epoch_num)\n",
        "    print('    epoch_loss =', epoch_total_loss)\n",
        "    print('    avg_epoch_loss =', avg_loss)\n",
        "    print('    learning rate =', optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "end = perf_counter()\n",
        "bert_training_time = end- start\n",
        "print('Training completed in ', bert_training_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WxtKGm3LQu_"
      },
      "source": [
        "### Testing\n",
        "Now that we trained the model, we can predict unseen examples on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kFlKiEDLSrd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b11716d184e94f73a155a1ca7af75b90",
            "691f5be932334b2086d4f4b9cf762a01",
            "862f014624354148a8fc31aca035619c",
            "6626ccf6b4ec4eec82155cb9f99714fb",
            "7eb05c94ebf44ed7b0498cf0e50eacff",
            "847e72b2a3494195975d51792cdf90ad",
            "709baba96d894b80a8c395598e325a8a",
            "65965caa0baa4b5f811ffb2a55566c5f",
            "aaf2e15f2da64f8e93814544ec3bed4d",
            "4454b222760a4164a32583936154adfa",
            "a0f50c19e79f42e2b3ac9fe117ca4c18"
          ]
        },
        "outputId": "783b4a59-4f86-4695-c911-42a462361fac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b11716d184e94f73a155a1ca7af75b90"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "bert_prediction_results = []\n",
        "\n",
        "# Define the test dataset similarly as above\n",
        "test_dataset = TextDataset(df=df_test, label_to_id=label_to_id, text_field='text', label_field='label')\n",
        "\n",
        "# Since we just perform prediction, we don't need to randomly sample the dataset\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            sampler=test_sampler)\n",
        "\n",
        "\n",
        "for batch in tqdm(test_dataloader):\n",
        "  # Put model in eval model: disable special features\n",
        "  bert_model.eval()\n",
        "\n",
        "  b_text, b_labels = batch\n",
        "  # Tokenizer description\n",
        "  b_inputs = bert_tokenizer(list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "  # Put to GPU\n",
        "  b_labels = b_labels.to(device)\n",
        "  b_inputs = b_inputs.to(device)\n",
        "\n",
        "  # tells pytorch to not calculate gradients since we won't be performing optimizer step\n",
        "  with torch.no_grad():\n",
        "      # Excerise: pass the inputs to the model and get logits\n",
        "      b_logits = ...\n",
        "      # Put logits to cpu (need to call detach before)\n",
        "      b_logits = b_logits.detach().cpu()\n",
        "\n",
        "  # Excerise: calculate the most likely predicted class given the output logits\n",
        "  # Tips: you can use torch.argmax()\n",
        "  batch_prediction = ...\n",
        "  bert_prediction_results.extend(batch_prediction.tolist())\n",
        "\n",
        "bert_prediction_labels = [id_to_label[p] for p in bert_prediction_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRXHSOn89rle"
      },
      "source": [
        "Generate the classification report by comparing the predictions from the model with the true labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VKAOiU1MC9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "defa82de-db8f-4516-d08a-f36dfba9c415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.87\n"
          ]
        }
      ],
      "source": [
        "bert_class_report = classification_report(df_test['label'], bert_prediction_labels, output_dict=True)\n",
        "bert_class_report['training_time (seconds)'] = bert_training_time\n",
        "\n",
        "with open(RESULTS_FOLDER + 'bert_class_report.json', 'w') as f:\n",
        "  json.dump(bert_class_report, f)\n",
        "\n",
        "print(bert_class_report['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNqZKydH9rlf"
      },
      "outputs": [],
      "source": [
        "# while True:pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiZKX8V6OUGB"
      },
      "source": [
        "## BERT + ResNet-50\n",
        "The next model that we are training uses a combination of BERT and ResNet-50 to encode the text and images, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B_AweX0Of5J"
      },
      "source": [
        "### Dataset\n",
        "Unlike the previous Dataset used for BERT, we include images in this dataset by reading the image files and applying a series of transformations to them so that they can fit into the ResNet model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvfc8ug3noFk"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, df, label_to_id, train=False, text_field=\"text\", label_field=\"label\", image_path_field=\"img_path\", img_size=224):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_to_id = label_to_id\n",
        "        self.train = train\n",
        "        self.text_field = text_field\n",
        "        self.label_field = label_field\n",
        "        self.image_path_field = image_path_field\n",
        "\n",
        "        # Default ResNet-50 settings. Do not change this\n",
        "        self.img_size = img_size # Pixel size\n",
        "        self.mean, self.std = (\n",
        "            0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711) # Average mean/std of pixels in the dataset\n",
        "\n",
        "        # We will perform some classic data augmentation for images during training\n",
        "        self.train_transform_func = transforms.Compose(\n",
        "                [transforms.RandomResizedCrop(self.img_size, scale=(0.5, 1.0)),  # Crops an image and rescale the crop images to the given size\n",
        "                    transforms.RandomHorizontalFlip(), # Randomly flip the image horizontally\n",
        "                    transforms.ToTensor(), # Put the image in tensor class\n",
        "                    transforms.Normalize(self.mean, self.std) # Normalize the pixel values\n",
        "                    ])\n",
        "\n",
        "        # During evaluation, we do not use data augmentation, but we make sure that the images have the same size as training images\n",
        "        self.eval_transform_func = transforms.Compose(\n",
        "                [transforms.Resize(256), # Resize the image to be of size (256,256) pixels\n",
        "                    transforms.CenterCrop(self.img_size), # Puts the image to the same size as training size\n",
        "                    transforms.ToTensor(), # # Put the image in tensor class\n",
        "                    transforms.Normalize(self.mean, self.std) # Normalize the pixel values\n",
        "                    ])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ## Exercise: fill the blank such that we get the right inputs\n",
        "        text = ...\n",
        "        # Do not forget to use the label_to_ids here\n",
        "        label = ...\n",
        "        # This should be the path to the desired image\n",
        "        img_path = IMAGE_FOLDER + ...\n",
        "\n",
        "        # Opens the desired image\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if self.train:\n",
        "          # Apply the training transformations\n",
        "          img = ...\n",
        "        else:\n",
        "          # Apply the evaluation transformations\n",
        "          img = ...\n",
        "\n",
        "        return text, label, img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoDHIiyxOjDf"
      },
      "source": [
        "### Model\n",
        "The original ResNet model consists of a fully connected layer with 1000 classes at the end, to show the scores of each image belonging to that class. However, our output classes are different and we want to use the image features before the fully connected layer instead of the 1000-class output probabilities. Therefore, we \"extract\" this model out of the original ResNet model architecture by leaving out the fully connected layer.\n",
        "\n",
        "\n",
        "After that, we pair the extracted ResNet model with a BERT model and add a 10-class linear layer on top of them, like we did for the previous BERT classifier.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1vFL3V1LdRlamLjkoI7ieoimxbwGnR7mU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKAYDZEw9rli"
      },
      "source": [
        "The ResNet-50 model is trained on imagenet data to classify images into 1000 classes, therefore the last layer is a fully connected layer with 1000 output nodes. This output is not useful to us since our output classes are different. Therefore, we need to strip off this fully connected layer and use the features after the last average pooling layer. This can be done by copying the layers and weights to another network and leave out the last layer.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1ivYlubrhvY00P7b2SYLfpRSF3XxJUbfh)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at the ResNet 50 model\n",
        "from torchvision.models.resnet import resnet50\n",
        "\n",
        "pretrained_resnet = resnet50(pretrained=True)"
      ],
      "metadata": {
        "id": "r6_ibJWmp7Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  pretrained_resnet = resnet50(pretrained=True)\n",
        "  children_list = []\n",
        "  for n,c in pretrained_resnet.named_children():\n",
        "      print(n)"
      ],
      "metadata": {
        "id": "fcFdEVtDp-c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_Svc1HgYPxW"
      },
      "outputs": [],
      "source": [
        "# extract layers of resnet-50 to build a new model\n",
        "\n",
        "import torch.nn as nn\n",
        "from torchvision.models.resnet import resnet50\n",
        "\n",
        "class ResNetFeatureModel(nn.Module):\n",
        "    \"\"\"A PyTorch ResNet wrapper that outputs the features derived from the `output_layer` layer.\"\"\"\n",
        "    def __init__(self, output_layer):\n",
        "        \"\"\"Parameters:\n",
        "        `output_layer: string` layer name of ResNet50 from which we want to derive features.\"\"\"\n",
        "        super().__init__()\n",
        "        self.output_layer = output_layer\n",
        "        pretrained_resnet = resnet50(pretrained=True)\n",
        "\n",
        "        self.children_list = []\n",
        "\n",
        "        # Exercise: build a loop that appends every layer up to `output_layer` (included) in self.children_list\n",
        "        ...\n",
        "\n",
        "        # The final network is just a sequential step of all layers in self.children_list\n",
        "        self.net = nn.Sequential(*self.children_list)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"Takes as input an image x, and outputs the image embeddings\"\"\"\n",
        "        # x shape: [batch_size, num_channels, img_size, img_size]\n",
        "        x = self.net(x)\n",
        "        # Exercise: what is the shape of x before and after flatten?\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aSXjPsesrzn"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "b5GzS6y8pVYu"
      },
      "outputs": [],
      "source": [
        "# last output layer name for resnet is named 'layer4', dim 2048*7*7\n",
        "# last layer name before fc is named 'avgpool', dim 2048*1*1 -> needs to be flattened\n",
        "\n",
        "class BertResNetModel(nn.Module):\n",
        "    \"\"\"PyTorch model that takes as input an image and its descriptions. Both are processed with ResNet50 and BERT respectively, and then concatenated.\"\"\"\n",
        "    def __init__(self, num_labels, text_pretrained='bert-base-uncased'):\n",
        "        super().__init__()\n",
        "        self.text_encoder = AutoModel.from_pretrained(text_pretrained)\n",
        "        # Fill the blank with the right output layer\n",
        "        self.visual_encoder = ResNetFeatureModel(output_layer=...)\n",
        "        self.image_hidden_size = 2048\n",
        "        # Fil the blank with the right embedding size\n",
        "        self.classifier = nn.Linear(..., num_labels)\n",
        "\n",
        "    def forward(self, text, image):\n",
        "        \"\"\"Given an image and its description, process each of them independently, concatenate their embedding and returns class likelihood.\"\"\"\n",
        "\n",
        "        # Text tower\n",
        "        text_output = self.text_encoder(input_ids=..., attention_mask=...)\n",
        "        text_feature = text_output.last_hidden_state[:, ..., :]\n",
        "\n",
        "        # Image tower\n",
        "        img_feature = self.visual_encoder(image)\n",
        "\n",
        "        # Final embeddings\n",
        "        # Excerise: concatenate the text and image features on the embedding axis\n",
        "        features = ...\n",
        "\n",
        "        # Likelihood with the linear layer\n",
        "        logits = self.classifier(features)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnrdfEhhp3QX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dde241a-d7db-4bc5-a577-6a5c462da5d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "resnet_model = BertResNetModel(num_labels=num_out_labels, text_pretrained='bert-base-uncased')\n",
        "resnet_model = resnet_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbVr5GyFOmUe"
      },
      "source": [
        "### Training\n",
        "Similar to BERT training, but we take in images as an additional input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snzv98OZom95"
      },
      "outputs": [],
      "source": [
        "## training loop\n",
        "set_seed(seed_val)\n",
        "\n",
        "# Define the dataset and the DataLoader\n",
        "train_dataset = ImageDataset(df=df_train, label_to_id=label_to_id, train=True, text_field='text', label_field='label', image_path_field='img_path', img_size=224)\n",
        "# The DataLoader will take care of separating the dataset in batches of size `batch_size`\n",
        "# The RandomSampler will take care of randomly splitting the data\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    sampler=train_sampler)\n",
        "\n",
        "\n",
        "# Total number of steps\n",
        "t_total = len(train_dataloader) * num_train_epochs\n",
        "\n",
        "# We use the AdamW optimizer here\n",
        "optimizer = AdamW(bert_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "# We also use a cosine scheduler: it will take care of lower the learning rate after each optimizer step.\n",
        "# Learning rate scheduling is often used to reduce overfitting on data\n",
        "scheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "\n",
        "# Here, we will use a cross entropy loss because it's a multiclass problem\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Put the model in train mode: (enables special features such as dropout, batch normalization...)\n",
        "resnet_model.train()\n",
        "\n",
        "start = perf_counter()\n",
        "# Run over all epochs\n",
        "for epoch_num in trange(num_train_epochs, desc='Epochs'):\n",
        "    epoch_total_loss = 0\n",
        "    # For each epoch, run over all batches of data\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc='Batch'):\n",
        "        b_text, b_labels, b_imgs = batch\n",
        "        # Tokenize the image description\n",
        "        b_inputs = bert_tokenizer(\n",
        "            list(b_text), truncation=True, max_length=max_seq_length,\n",
        "            return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "        # Put input data to GPU\n",
        "        b_labels = b_labels.to(device)\n",
        "        b_imgs = b_imgs.to(device)\n",
        "        b_inputs = b_inputs.to(device)\n",
        "\n",
        "        # Exercise: fill the pytorch training loop\n",
        "\n",
        "        ## Step 1: gradient should be set to 0\n",
        "        ...\n",
        "        ## Step 2: pass the inputs to the model and get logits\n",
        "        b_logits = ...\n",
        "        ## Step 3: Calculate loss given the logits and the labels\n",
        "        loss = ...\n",
        "\n",
        "        ## accumulates all losses\n",
        "        epoch_total_loss += loss.item()\n",
        "\n",
        "        ## Perform a backward pass to calculate the gradients\n",
        "        ...\n",
        "\n",
        "        ## Perform the optimizer step: backpropagation on model parameters\n",
        "        ...\n",
        "        ## Perform the scheduelr step: lowers the learning rate according to the schedule\n",
        "        ...\n",
        "\n",
        "    avg_loss = epoch_total_loss/len(train_dataloader)\n",
        "\n",
        "\n",
        "    print('epoch =', epoch_num)\n",
        "    print('    epoch_loss =', epoch_total_loss)\n",
        "    print('    avg_epoch_loss =', avg_loss)\n",
        "    print('    learning rate =', optimizer.param_groups[0][\"lr\"])\n",
        "end = perf_counter()\n",
        "resnet_training_time = end- start\n",
        "print('Training completed in ', resnet_training_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXy-pTr1OqFX"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VPROY_Jr8AQ"
      },
      "outputs": [],
      "source": [
        "# testing loop\n",
        "\n",
        "resnet_prediction_results = []\n",
        "\n",
        "# Define the test dataset similarly as above\n",
        "test_dataset = ImageDataset(df=df_test, label_to_id=label_to_id, train=False, text_field='text', label_field='label', image_path_field='img_path', img_size=224)\n",
        "# Since we just perform prediction, we don't need to randomly sample the dataset\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            sampler=test_sampler)\n",
        "\n",
        "# Put model in eval model: disable special features\n",
        "resnet_model.eval()\n",
        "\n",
        "for batch in tqdm(test_dataloader):\n",
        "\n",
        "  b_text, b_labels, b_imgs = batch\n",
        "  # Tokenize description\n",
        "  b_inputs = bert_tokenizer(list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True)\n",
        "  # Put to GPU\n",
        "  b_labels = b_labels.to(device)\n",
        "  b_imgs = b_imgs.to(device)\n",
        "  b_inputs = b_inputs.to(device)\n",
        "\n",
        "  # tells pytorch to not calculate gradients since we won't be performing optimizer step\n",
        "  with torch.no_grad():\n",
        "      # Excerise: pass the inputs to the model and get logits\n",
        "\n",
        "      b_logits = ...\n",
        "      b_logits = b_logits.detach().cpu()\n",
        "\n",
        "  # Excerise: calculate the most likely predicted class given the output logits\n",
        "  # Tips: you can use torch.argmax()\n",
        "  batch_prediction = ...\n",
        "  resnet_prediction_results += batch_prediction.tolist()\n",
        "\n",
        "resnet_prediction_labels = [id_to_label[p] for p in resnet_prediction_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Stsa3WC9rlo"
      },
      "source": [
        "Generate the classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIGirJocsLjv"
      },
      "outputs": [],
      "source": [
        "resnet_class_report = classification_report(df_test['label'], resnet_prediction_labels, output_dict=True)\n",
        "resnet_class_report['training_time (seconds)'] = resnet_training_time\n",
        "\n",
        "with open(RESULTS_FOLDER + 'resnet_class_report.json', 'w') as f:\n",
        "  json.dump(resnet_class_report, f)\n",
        "\n",
        "print(resnet_class_report['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2qdA-QH9rlp"
      },
      "outputs": [],
      "source": [
        "# while True:pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24cKM-7yFX3o"
      },
      "source": [
        "## ALBEF\n",
        "The last model that we are training is the ALBEF joint-encoder model which aligns the text and image features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xfvf5g9DJGn"
      },
      "source": [
        "### ALBEF-specific setup\n",
        "This section creates the folder structure and download the necessary files required to train an ALBEF model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WZGS6gtILdzI"
      },
      "outputs": [],
      "source": [
        "ALBEF_FOLDER = HOME_FOLDER + 'ALBEF/'\n",
        "os.makedirs(ALBEF_FOLDER, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh3I4VxfDFQc"
      },
      "outputs": [],
      "source": [
        "# download pre-trained ALBEF model and required ALBEF files from ALBEF's official repo (only need to do this once to save it in your gdrive)\n",
        "!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/vit.py -O $ALBEF_FOLDER/vit.py\n",
        "!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/tokenization_bert.py -O $ALBEF_FOLDER/tokenization_bert.py\n",
        "!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/xbert.py -O $ALBEF_FOLDER/xbert.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Gix7rI5Cjtv8"
      },
      "outputs": [],
      "source": [
        "# replace all occurrences of tokenizer_class with processor_class in xbert.py to make it compatible with newer transformers version\n",
        "# if you don't do this step, you will need to install transformers==4.8.1 as specified by the requirements in the ALBEF repo\n",
        "\n",
        "!sed -i 's/tokenizer_class/processor_class/g' $ALBEF_FOLDER/xbert.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogf5vTJ8E4jB"
      },
      "outputs": [],
      "source": [
        "# add path to downloaded ALBEF files\n",
        "import sys\n",
        "sys.path.append(ALBEF_FOLDER)\n",
        "\n",
        "#import libraries required for ALBEF\n",
        "from vit import VisionTransformer\n",
        "from xbert import BertConfig as AlbefBertConfig, BertModel as AlbefBertModel\n",
        "from functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p675p6mOElsJ"
      },
      "source": [
        "### Dataset\n",
        "Same as the BERT-ResNet Dataset which contains **text**, **images** and **labels**. The only difference here is the image size (ResNet - 224, ALBEF - 256)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfUMju-XFzgs"
      },
      "source": [
        "### Model\n",
        "ALBEF also uses different encoders:\n",
        "- **Text Encoder**: BERT\n",
        "- **Image Encoder**: VisionTransformer\n",
        "\n",
        "We use the joint text-image encoder to encode both the text and images, and as with the previous two models, add a linear fully connected layer to it.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1zcBBx08_7ujlH2RS2WZrmTZ--Icsk4NN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avDhnSt_ogd1"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2wXdpo6bIIt"
      },
      "outputs": [],
      "source": [
        "class AlbefModel(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_config, num_labels, text_pretrained='bert-base-uncased'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_labels = num_labels\n",
        "        # Loads the pretrained ALBEF bert model\n",
        "        self.text_encoder = AlbefBertModel.from_pretrained(\n",
        "            text_pretrained, config=bert_config, add_pooling_layer=False)\n",
        "        # Loads Vision transformer with some default parameters.\n",
        "        # You can play with the following parameters to see how the differences: `embed_dim`, `depth`, `num_heads`, `mlp_ratio`.\n",
        "        # You can check here for a description of hyperparameters: https://github.com/lucidrains/vit-pytorch?tab=readme-ov-file#vision-transformer---pytorch\n",
        "        ### Exercise: what should be the embed_dim?\n",
        "        ### Tip: look at the ALBER images above\n",
        "        self.visual_encoder = VisionTransformer(\n",
        "            img_size=256, patch_size=16, embed_dim=..., depth=12, num_heads=12,\n",
        "            mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n",
        "\n",
        "        # Exericse: Fill the blank with the right embedding size\n",
        "        self.classifier = nn.Linear(\n",
        "            ..., num_labels)\n",
        "\n",
        "\n",
        "    def forward(self, text, image):\n",
        "        # We start by processing the images with the Vision Transformer\n",
        "        # Exercise: what shape will be the output?\n",
        "        # answer: [batch_size, img_size + 1, 768]\n",
        "        image_embeds = self.visual_encoder(image)\n",
        "\n",
        "        # Builds the attention masks of image for cross-attention with text embeddings\n",
        "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image_embeds.device)\n",
        "\n",
        "        # ALBEF uses cross-attention to combine image embeddings and text embeddings\n",
        "        # It's quite easy to implement it: just pass the image embeddings/masks in the encoder_ fields.\n",
        "        output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n",
        "                                   encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, return_dict=True\n",
        "                                   )\n",
        "\n",
        "        # Similarly as BERT, use the representation of the [CLS] token to do classification\n",
        "        logits = self.classifier(output.last_hidden_state[:, ..., :])\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE2Q63lL9rlw"
      },
      "source": [
        "Because ALBEF aligns the BERT and VisionTransformers features, it has its own BERT configuration. We download both this configuration and the pretrained model from Salesforce's GitHub and web pages in the function below which loads a pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cMGq71HN9rlw"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlretrieve\n",
        "\n",
        "def load_albef_pretrained(num_out_labels):\n",
        "    \"\"\"Loads pretrained ALBEF by downloading the right configuration and pretrained weights.\"\"\"\n",
        "    tmp_directory = './tmp/albef'\n",
        "    os.makedirs(tmp_directory, exist_ok=True)\n",
        "\n",
        "    albef_bert_config_fp = os.path.join(tmp_directory, 'config_bert.json')\n",
        "    albef_model_fp = os.path.join(tmp_directory, 'ALBEF.pth')\n",
        "\n",
        "    if not os.path.exists(albef_bert_config_fp):\n",
        "        urlretrieve(\"https://raw.githubusercontent.com/salesforce/ALBEF/main/configs/config_bert.json\", albef_bert_config_fp)\n",
        "\n",
        "    if not os.path.exists(albef_model_fp):\n",
        "        urlretrieve(\"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF_4M.pth\", albef_model_fp)\n",
        "\n",
        "    albef_bert_config = AlbefBertConfig.from_json_file(albef_bert_config_fp)\n",
        "    albef_model = AlbefModel(bert_config=albef_bert_config, num_labels=num_out_labels)\n",
        "\n",
        "    albef_checkpoint = torch.load(albef_model_fp, map_location='cpu')\n",
        "    albef_state_dict = albef_checkpoint['model']\n",
        "\n",
        "    for key in list(albef_state_dict.keys()):\n",
        "        if 'bert' in key:\n",
        "            encoder_key = key.replace('bert.', '')\n",
        "            albef_state_dict[encoder_key] = albef_state_dict[key]\n",
        "            del albef_state_dict[key]\n",
        "\n",
        "    msg = albef_model.load_state_dict(albef_state_dict, strict=False)\n",
        "    print(\"ALBEF checkpoint loaded from \", albef_model_fp)\n",
        "    print(msg)\n",
        "    return albef_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kwWc1JU9rlx"
      },
      "outputs": [],
      "source": [
        "albef_model = load_albef_pretrained(num_out_labels=10)\n",
        "albef_model = albef_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lMzlczHF7vf"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7i__SQkejJ1v"
      },
      "outputs": [],
      "source": [
        "## training loop\n",
        "set_seed(seed_val)\n",
        "\n",
        "train_dataset = ImageDataset(df=df_train, label_to_id=label_to_id, train=True, text_field='text', label_field='label', image_path_field='img_path', image_size=256)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    sampler=train_sampler)\n",
        "\n",
        "\n",
        "t_total = len(train_dataloader) * num_train_epochs\n",
        "\n",
        "\n",
        "optimizer = AdamW(albef_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "scheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "albef_model.train()\n",
        "\n",
        "start = perf_counter()\n",
        "for epoch_num in trange(num_train_epochs, desc='Epochs'):\n",
        "    epoch_total_loss = 0\n",
        "\n",
        "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc='Batch'):\n",
        "        b_text, b_labels, b_imgs = batch\n",
        "        b_inputs = bert_tokenizer(\n",
        "            list(b_text), truncation=True, max_length=max_seq_length,\n",
        "            return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "\n",
        "        b_labels = b_labels.to(device)\n",
        "        b_imgs = b_imgs.to(device)\n",
        "        b_inputs = b_inputs.to(device)\n",
        "\n",
        "        # Exercise: fill the pytorch training loop\n",
        "\n",
        "        ## Step 1: gradient should be set to 0\n",
        "        ...\n",
        "        ## Step 2: pass the inputs to the model and get logits\n",
        "        b_logits = ...\n",
        "        ## Step 3: Calculate loss given the logits and the labels\n",
        "        loss = ...\n",
        "\n",
        "        ## accumulates all losses\n",
        "        epoch_total_loss += loss.item()\n",
        "\n",
        "        ## Perform a backward pass to calculate the gradients\n",
        "        ...\n",
        "\n",
        "        ## Perform the optimizer step: backpropagation on model parameters\n",
        "        ...\n",
        "        ## Perform the scheduelr step: lowers the learning rate according to the schedule\n",
        "        ...\n",
        "\n",
        "    avg_loss = epoch_total_loss/len(train_dataloader)\n",
        "\n",
        "\n",
        "    print('epoch =', epoch_num)\n",
        "    print('    epoch_loss =', epoch_total_loss)\n",
        "    print('    avg_epoch_loss =', avg_loss)\n",
        "    print('    learning rate =', optimizer.param_groups[0][\"lr\"])\n",
        "end = perf_counter()\n",
        "albef_training_time = end- start\n",
        "print('Training completed in ', albef_training_time, 'seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGt9T1n2F-zm"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5_062z-TWJf"
      },
      "outputs": [],
      "source": [
        "# testing loop\n",
        "\n",
        "albef_prediction_results = []\n",
        "\n",
        "test_dataset = ImageDataset(df=df_test, label_to_id=label_to_id, train=False, text_field='text', label_field='label', image_path_field='img_path')\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            sampler=test_sampler)\n",
        "\n",
        "\n",
        "for batch in tqdm(test_dataloader):\n",
        "  albef_model.eval()\n",
        "\n",
        "  b_text, b_labels, b_imgs = batch\n",
        "\n",
        "  b_inputs = bert_tokenizer(list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "  b_labels = b_labels.to(device)\n",
        "  b_imgs = b_imgs.to(device)\n",
        "  b_inputs = b_inputs.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      # Excerise: pass the inputs to the model and get logits\n",
        "\n",
        "      b_logits = ...\n",
        "      b_logits = b_logits.detach().cpu()\n",
        "\n",
        "  # Excerise: calculate the most likely predicted class given the output logits\n",
        "  # Tips: you can use torch.argmax()\n",
        "  batch_prediction = ...\n",
        "  albef_prediction_results += batch_prediction.tolist()\n",
        "\n",
        "albef_prediction_labels = [id_to_label[p] for p in albef_prediction_results]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8b7U2b39rlz"
      },
      "source": [
        "Generate the classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl-_hFYYZWk7"
      },
      "outputs": [],
      "source": [
        "albef_class_report = classification_report(df_test['label'], albef_prediction_labels, output_dict=True)\n",
        "albef_class_report['training_time (seconds)'] = albef_training_time\n",
        "\n",
        "with open(RESULTS_FOLDER + 'albef_class_report.json', 'w') as f:\n",
        "  json.dump(albef_class_report, f)\n",
        "\n",
        "print(albef_class_report['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFmus7PzUTtR"
      },
      "source": [
        "## Predict on models trained with 20 epochs\n",
        "In the previous segments, we trained each model for only 5 epochs due to the tutorial's time constraint. Thus, we cannot see a significant contrast between the accuracies of the models. Training for more epochs will improve the models' accuracies. Therefore, we have trained the models for 20 epochs each and saved them. In this segment, we will load the models and make predictions on the test set to compare their accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw5dqiee9rl0"
      },
      "source": [
        "The code in the previous segments have to be reused to load the models. Before executing this step, the following cells must have been executed:\n",
        "- Setup and common config cells\n",
        "- ALBEF-specific cells\n",
        "- ALBEF-loading cells\n",
        "- Cells containing model code for BERT, BERT-ResNet and ALBEF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dF6CuOIe9rl1"
      },
      "outputs": [],
      "source": [
        "# Download trained_models.zip file to trained_models folder\n",
        "!gdown 'https://drive.google.com/uc?id=1r-tHlbxIeajopWCNKMtnQVvvXdyuM8WC' -O $HOME_FOLDER/trained_models.zip\n",
        "!unzip $HOME_FOLDER/trained_models.zip -d $HOME_FOLDER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osKv4stl9rl2"
      },
      "source": [
        "This function loads the pretrained model for each of the three model architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XADok_mB9rl2"
      },
      "outputs": [],
      "source": [
        "def load_trained_models(load_directory, image_model_type):\n",
        "    \"\"\"Loads the pretrained model for each of the three model architectures.\"\"\"\"\n",
        "    label_map_filepath = os.path.join(load_directory, \"label_map.json\")\n",
        "    with open(label_map_filepath, 'r') as f:\n",
        "        label_to_id = json.load(f)\n",
        "\n",
        "    id_to_label = {v:k for k,v in label_to_id.items()}\n",
        "\n",
        "    num_labels = len(label_to_id)\n",
        "\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    model_sd_filepath = os.path.join(load_directory, \"state_dict.pt\")\n",
        "    model_sd = torch.load(model_sd_filepath, map_location='cpu')\n",
        "\n",
        "    if image_model_type is None:\n",
        "        model = VLBertModel(num_labels=num_labels)\n",
        "    elif image_model_type.lower() == 'resnet':\n",
        "        model = BertResNetModel(num_labels=num_labels)\n",
        "    elif image_model_type.lower() == 'albef':\n",
        "        model = load_albef_pretrained(num_out_labels=num_labels)\n",
        "\n",
        "    model.to('cpu') # load all models in cpu first\n",
        "    model.load_state_dict(model_sd, strict=True)\n",
        "    model.to(device)\n",
        "\n",
        "    return model, tokenizer, label_to_id, id_to_label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdhO1Dpw9rl3"
      },
      "source": [
        "We streamline the three different datasets presented previously into one common VLDataset class which has **text**, **images** and **labels**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dFHDu3X9rl4"
      },
      "outputs": [],
      "source": [
        "class VLDataset(Dataset):\n",
        "    \"\"\"Unified PyTorch Dataset that works for both text only and text + images\"\"\"\n",
        "    def __init__(self, df, label_to_id, train=False, text_field=\"text\", label_field=\"label\", image_path_field=None, image_model_type=None):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_to_id = label_to_id\n",
        "        self.train = train\n",
        "        self.text_field = text_field\n",
        "        self.label_field = label_field\n",
        "        self.image_path_field = image_path_field\n",
        "        self.image_model_type = image_model_type\n",
        "\n",
        "        # text only dataset\n",
        "        if image_model_type is not None:\n",
        "\n",
        "            # ResNet-50 and ALBEF use different image sizes: fill the blanks\n",
        "            if image_model_type.lower() == \"resnet\":   # ResNet-50 settings\n",
        "                self.img_size = ...\n",
        "            elif image_model_type.lower() == \"albef\":   # ALBEF settings\n",
        "                self.img_size = ...\n",
        "\n",
        "            self.mean, self.std = (\n",
        "                0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)\n",
        "\n",
        "\n",
        "            self.train_transform_func = transforms.Compose(\n",
        "                    [transforms.RandomResizedCrop(self.img_size, scale=(0.5, 1.0)),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(self.mean, self.std)\n",
        "                        ])\n",
        "\n",
        "            self.eval_transform_func = transforms.Compose(\n",
        "                    [transforms.Resize(256),\n",
        "                        transforms.CenterCrop(self.img_size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize(self.mean, self.std)\n",
        "                        ])\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ## Exercise: fill this function such that it returns the right output depending on if its text only or text+image\n",
        "        ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBga2Dt-9rl5"
      },
      "source": [
        "We also streamline the predict function to do prediction on the test set with the loaded models of any of the three model architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcnQRXbn9rl5"
      },
      "outputs": [],
      "source": [
        "## testing loop\n",
        "def predict(df_test, model, tokenizer, label_to_id, id_to_label, image_model_type):\n",
        "    prediction_results = []\n",
        "\n",
        "    test_dataset = VLDataset(df=df_test, label_to_id=label_to_id, train=False, text_field='text', label_field='label', image_path_field='img_path', image_model_type=image_model_type)\n",
        "    test_sampler = SequentialSampler(test_dataset)\n",
        "    test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                sampler=test_sampler)\n",
        "\n",
        "\n",
        "    for batch in tqdm(test_dataloader):\n",
        "        model.eval()\n",
        "\n",
        "        if image_model_type is None:\n",
        "          b_text, b_labels = batch\n",
        "          b_imgs = None\n",
        "        else:\n",
        "          b_text, b_labels, b_imgs = batch\n",
        "\n",
        "        b_inputs = tokenizer(list(b_text), truncation=True, max_length=max_seq_length, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        b_labels = b_labels.to(device)\n",
        "        b_inputs = b_inputs.to(device)\n",
        "\n",
        "        if b_imgs is not None:\n",
        "          b_imgs = b_imgs.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Excerise: pass the inputs to the model and get logits\n",
        "            if b_imgs is not None:\n",
        "              b_logits = ...\n",
        "            else:\n",
        "              b_logits = ...\n",
        "\n",
        "            b_logits = b_logits.detach().cpu()\n",
        "\n",
        "        # Excerise: calculate the most likely predicted class given the output logits\n",
        "        # Tips: you can use torch.argmax()\n",
        "        prediction_results = ...\n",
        "        prediction_results += prediction_results.tolist()\n",
        "\n",
        "    prediction_labels = [id_to_label[p] for p in prediction_results]\n",
        "\n",
        "    print(accuracy_score(df_test['label'], prediction_labels))\n",
        "\n",
        "    return prediction_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x4FnNTT9rl6"
      },
      "source": [
        "### Predict with loaded BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TjgTE7L9rl6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "961aeaad14a24a00af0c38ac1862f052",
            "903cded981e84632b43d4c9953a89fd4",
            "c836463c123e4b3489225d4c8d31135a",
            "a0e1ace638974af195bf5ccb253342a3",
            "85e85d1319d444b695d8f84464599818",
            "97c6d30f53884a47bdf2e0c5783c4632",
            "8dceed58ac444454853fa9de178159fb",
            "da5689c39b2e4da2b65b810ae3fc3d5a",
            "5f180c827c4c4820a65a9bc2cc3be1ae",
            "b0684d9797c44e83a0d57e8ffbf0f26e",
            "03e0663e4b764cee86304f43e12d94ff"
          ]
        },
        "outputId": "4142d78c-82ff-4cce-f332-86b9cb09acca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-b9faa2ee2f48>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_sd = torch.load(model_sd_filepath, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "961aeaad14a24a00af0c38ac1862f052"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.87\n"
          ]
        }
      ],
      "source": [
        "bert_load_directory = TRAINED_MODELS_FOLDER + 'BERT'\n",
        "bert_model, bert_tokenizer, label_to_id, id_to_label = load_trained_models(bert_load_directory, image_model_type=None)\n",
        "bert_predictions = predict(df_test.copy(), bert_model, bert_tokenizer, label_to_id, id_to_label, image_model_type=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLCrcmRA9rl7"
      },
      "source": [
        "### Predict with loaded BERT-ResNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iC4aXvp09rl7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "258c3344438244bda943b63456e55bbe",
            "9f888deb0a284dbc9b1bd7fac73a66b7",
            "37ee1a4884934e4fbe636c6d0443622a",
            "a1e0e47a1b9c4114a2f789b957f587c5",
            "40057e4b0ac5408da574adfb9dd6f1f3",
            "9c2061b5ac4a4196a5c0979b2db85a27",
            "a031d36f9c89433b908f1654405225e9",
            "c174102d5341435d92dafd964ee5a9e6",
            "2b0940e48b4d4471923a578ad82c7514",
            "b1cca46377c1475a9e294706e2aa6e99",
            "dc95ef9b8dc54b00b940f4857dd88c85"
          ]
        },
        "outputId": "0047a608-7e58-4ece-c80e-3850c70861d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-b9faa2ee2f48>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_sd = torch.load(model_sd_filepath, map_location='cpu')\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "258c3344438244bda943b63456e55bbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.905\n"
          ]
        }
      ],
      "source": [
        "bert_resnet_load_directory = TRAINED_MODELS_FOLDER + 'BERT_ResNet'\n",
        "bert_resnet_model, bert_resnet_tokenizer, label_to_id, id_to_label = load_trained_models(bert_resnet_load_directory, image_model_type='resnet')\n",
        "bert_resnet_predictions = predict(df_test.copy(), bert_resnet_model, bert_resnet_tokenizer, label_to_id, id_to_label, image_model_type='resnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyXB0LIV9rl8"
      },
      "source": [
        "### Predict with loaded ALBEF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD9dUztH9rl8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "7c756f0cadfa463aa30c9d0c4b2a3b8d",
            "69677629740147e4b8f16a17117ba8ea",
            "56de997b6020440cae66920886fbd51b",
            "1910d86a931d4a5284cd9a7d9af2a666",
            "e91818fa67aa479780d848e41246731f",
            "a7d1543a8826417da134ebf4e8f7c5cb",
            "52edbad7017c4ad28487e9977294ec78",
            "7b1136ba4fb24c0786f27febc8caa3d6",
            "8ba9a244c50e4813a671502dc958245a",
            "07648340950f4463901de3d5c8cacff9",
            "36095b727d7c4931ad234b1f32f2c900"
          ]
        },
        "outputId": "42d3779b-4ede-479b-9471-77713614af8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-b9faa2ee2f48>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_sd = torch.load(model_sd_filepath, map_location='cpu')\n",
            "<ipython-input-85-310a5e74aa48>:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  albef_checkpoint = torch.load(albef_model_fp, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALBEF checkpoint loaded from  ./tmp/albef/ALBEF.pth\n",
            "_IncompatibleKeys(missing_keys=['classifier.weight', 'classifier.bias'], unexpected_keys=['temp', 'image_queue', 'text_queue', 'queue_ptr', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias', 'visual_encoder_m.cls_token', 'visual_encoder_m.pos_embed', 'visual_encoder_m.patch_embed.proj.weight', 'visual_encoder_m.patch_embed.proj.bias', 'visual_encoder_m.blocks.0.norm1.weight', 'visual_encoder_m.blocks.0.norm1.bias', 'visual_encoder_m.blocks.0.attn.qkv.weight', 'visual_encoder_m.blocks.0.attn.qkv.bias', 'visual_encoder_m.blocks.0.attn.proj.weight', 'visual_encoder_m.blocks.0.attn.proj.bias', 'visual_encoder_m.blocks.0.norm2.weight', 'visual_encoder_m.blocks.0.norm2.bias', 'visual_encoder_m.blocks.0.mlp.fc1.weight', 'visual_encoder_m.blocks.0.mlp.fc1.bias', 'visual_encoder_m.blocks.0.mlp.fc2.weight', 'visual_encoder_m.blocks.0.mlp.fc2.bias', 'visual_encoder_m.blocks.1.norm1.weight', 'visual_encoder_m.blocks.1.norm1.bias', 'visual_encoder_m.blocks.1.attn.qkv.weight', 'visual_encoder_m.blocks.1.attn.qkv.bias', 'visual_encoder_m.blocks.1.attn.proj.weight', 'visual_encoder_m.blocks.1.attn.proj.bias', 'visual_encoder_m.blocks.1.norm2.weight', 'visual_encoder_m.blocks.1.norm2.bias', 'visual_encoder_m.blocks.1.mlp.fc1.weight', 'visual_encoder_m.blocks.1.mlp.fc1.bias', 'visual_encoder_m.blocks.1.mlp.fc2.weight', 'visual_encoder_m.blocks.1.mlp.fc2.bias', 'visual_encoder_m.blocks.2.norm1.weight', 'visual_encoder_m.blocks.2.norm1.bias', 'visual_encoder_m.blocks.2.attn.qkv.weight', 'visual_encoder_m.blocks.2.attn.qkv.bias', 'visual_encoder_m.blocks.2.attn.proj.weight', 'visual_encoder_m.blocks.2.attn.proj.bias', 'visual_encoder_m.blocks.2.norm2.weight', 'visual_encoder_m.blocks.2.norm2.bias', 'visual_encoder_m.blocks.2.mlp.fc1.weight', 'visual_encoder_m.blocks.2.mlp.fc1.bias', 'visual_encoder_m.blocks.2.mlp.fc2.weight', 'visual_encoder_m.blocks.2.mlp.fc2.bias', 'visual_encoder_m.blocks.3.norm1.weight', 'visual_encoder_m.blocks.3.norm1.bias', 'visual_encoder_m.blocks.3.attn.qkv.weight', 'visual_encoder_m.blocks.3.attn.qkv.bias', 'visual_encoder_m.blocks.3.attn.proj.weight', 'visual_encoder_m.blocks.3.attn.proj.bias', 'visual_encoder_m.blocks.3.norm2.weight', 'visual_encoder_m.blocks.3.norm2.bias', 'visual_encoder_m.blocks.3.mlp.fc1.weight', 'visual_encoder_m.blocks.3.mlp.fc1.bias', 'visual_encoder_m.blocks.3.mlp.fc2.weight', 'visual_encoder_m.blocks.3.mlp.fc2.bias', 'visual_encoder_m.blocks.4.norm1.weight', 'visual_encoder_m.blocks.4.norm1.bias', 'visual_encoder_m.blocks.4.attn.qkv.weight', 'visual_encoder_m.blocks.4.attn.qkv.bias', 'visual_encoder_m.blocks.4.attn.proj.weight', 'visual_encoder_m.blocks.4.attn.proj.bias', 'visual_encoder_m.blocks.4.norm2.weight', 'visual_encoder_m.blocks.4.norm2.bias', 'visual_encoder_m.blocks.4.mlp.fc1.weight', 'visual_encoder_m.blocks.4.mlp.fc1.bias', 'visual_encoder_m.blocks.4.mlp.fc2.weight', 'visual_encoder_m.blocks.4.mlp.fc2.bias', 'visual_encoder_m.blocks.5.norm1.weight', 'visual_encoder_m.blocks.5.norm1.bias', 'visual_encoder_m.blocks.5.attn.qkv.weight', 'visual_encoder_m.blocks.5.attn.qkv.bias', 'visual_encoder_m.blocks.5.attn.proj.weight', 'visual_encoder_m.blocks.5.attn.proj.bias', 'visual_encoder_m.blocks.5.norm2.weight', 'visual_encoder_m.blocks.5.norm2.bias', 'visual_encoder_m.blocks.5.mlp.fc1.weight', 'visual_encoder_m.blocks.5.mlp.fc1.bias', 'visual_encoder_m.blocks.5.mlp.fc2.weight', 'visual_encoder_m.blocks.5.mlp.fc2.bias', 'visual_encoder_m.blocks.6.norm1.weight', 'visual_encoder_m.blocks.6.norm1.bias', 'visual_encoder_m.blocks.6.attn.qkv.weight', 'visual_encoder_m.blocks.6.attn.qkv.bias', 'visual_encoder_m.blocks.6.attn.proj.weight', 'visual_encoder_m.blocks.6.attn.proj.bias', 'visual_encoder_m.blocks.6.norm2.weight', 'visual_encoder_m.blocks.6.norm2.bias', 'visual_encoder_m.blocks.6.mlp.fc1.weight', 'visual_encoder_m.blocks.6.mlp.fc1.bias', 'visual_encoder_m.blocks.6.mlp.fc2.weight', 'visual_encoder_m.blocks.6.mlp.fc2.bias', 'visual_encoder_m.blocks.7.norm1.weight', 'visual_encoder_m.blocks.7.norm1.bias', 'visual_encoder_m.blocks.7.attn.qkv.weight', 'visual_encoder_m.blocks.7.attn.qkv.bias', 'visual_encoder_m.blocks.7.attn.proj.weight', 'visual_encoder_m.blocks.7.attn.proj.bias', 'visual_encoder_m.blocks.7.norm2.weight', 'visual_encoder_m.blocks.7.norm2.bias', 'visual_encoder_m.blocks.7.mlp.fc1.weight', 'visual_encoder_m.blocks.7.mlp.fc1.bias', 'visual_encoder_m.blocks.7.mlp.fc2.weight', 'visual_encoder_m.blocks.7.mlp.fc2.bias', 'visual_encoder_m.blocks.8.norm1.weight', 'visual_encoder_m.blocks.8.norm1.bias', 'visual_encoder_m.blocks.8.attn.qkv.weight', 'visual_encoder_m.blocks.8.attn.qkv.bias', 'visual_encoder_m.blocks.8.attn.proj.weight', 'visual_encoder_m.blocks.8.attn.proj.bias', 'visual_encoder_m.blocks.8.norm2.weight', 'visual_encoder_m.blocks.8.norm2.bias', 'visual_encoder_m.blocks.8.mlp.fc1.weight', 'visual_encoder_m.blocks.8.mlp.fc1.bias', 'visual_encoder_m.blocks.8.mlp.fc2.weight', 'visual_encoder_m.blocks.8.mlp.fc2.bias', 'visual_encoder_m.blocks.9.norm1.weight', 'visual_encoder_m.blocks.9.norm1.bias', 'visual_encoder_m.blocks.9.attn.qkv.weight', 'visual_encoder_m.blocks.9.attn.qkv.bias', 'visual_encoder_m.blocks.9.attn.proj.weight', 'visual_encoder_m.blocks.9.attn.proj.bias', 'visual_encoder_m.blocks.9.norm2.weight', 'visual_encoder_m.blocks.9.norm2.bias', 'visual_encoder_m.blocks.9.mlp.fc1.weight', 'visual_encoder_m.blocks.9.mlp.fc1.bias', 'visual_encoder_m.blocks.9.mlp.fc2.weight', 'visual_encoder_m.blocks.9.mlp.fc2.bias', 'visual_encoder_m.blocks.10.norm1.weight', 'visual_encoder_m.blocks.10.norm1.bias', 'visual_encoder_m.blocks.10.attn.qkv.weight', 'visual_encoder_m.blocks.10.attn.qkv.bias', 'visual_encoder_m.blocks.10.attn.proj.weight', 'visual_encoder_m.blocks.10.attn.proj.bias', 'visual_encoder_m.blocks.10.norm2.weight', 'visual_encoder_m.blocks.10.norm2.bias', 'visual_encoder_m.blocks.10.mlp.fc1.weight', 'visual_encoder_m.blocks.10.mlp.fc1.bias', 'visual_encoder_m.blocks.10.mlp.fc2.weight', 'visual_encoder_m.blocks.10.mlp.fc2.bias', 'visual_encoder_m.blocks.11.norm1.weight', 'visual_encoder_m.blocks.11.norm1.bias', 'visual_encoder_m.blocks.11.attn.qkv.weight', 'visual_encoder_m.blocks.11.attn.qkv.bias', 'visual_encoder_m.blocks.11.attn.proj.weight', 'visual_encoder_m.blocks.11.attn.proj.bias', 'visual_encoder_m.blocks.11.norm2.weight', 'visual_encoder_m.blocks.11.norm2.bias', 'visual_encoder_m.blocks.11.mlp.fc1.weight', 'visual_encoder_m.blocks.11.mlp.fc1.bias', 'visual_encoder_m.blocks.11.mlp.fc2.weight', 'visual_encoder_m.blocks.11.mlp.fc2.bias', 'visual_encoder_m.norm.weight', 'visual_encoder_m.norm.bias', 'vision_proj_m.weight', 'vision_proj_m.bias', 'text_encoder_m.cls.predictions.bias', 'text_encoder_m.cls.predictions.transform.dense.weight', 'text_encoder_m.cls.predictions.transform.dense.bias', 'text_encoder_m.cls.predictions.transform.LayerNorm.weight', 'text_encoder_m.cls.predictions.transform.LayerNorm.bias', 'text_encoder_m.cls.predictions.decoder.weight', 'text_encoder_m.cls.predictions.decoder.bias', 'text_proj_m.weight', 'text_proj_m.bias', 'text_encoder_m.embeddings.position_ids', 'text_encoder_m.embeddings.word_embeddings.weight', 'text_encoder_m.embeddings.position_embeddings.weight', 'text_encoder_m.embeddings.token_type_embeddings.weight', 'text_encoder_m.embeddings.LayerNorm.weight', 'text_encoder_m.embeddings.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.attention.self.query.weight', 'text_encoder_m.encoder.layer.0.attention.self.query.bias', 'text_encoder_m.encoder.layer.0.attention.self.key.weight', 'text_encoder_m.encoder.layer.0.attention.self.key.bias', 'text_encoder_m.encoder.layer.0.attention.self.value.weight', 'text_encoder_m.encoder.layer.0.attention.self.value.bias', 'text_encoder_m.encoder.layer.0.attention.output.dense.weight', 'text_encoder_m.encoder.layer.0.attention.output.dense.bias', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.0.intermediate.dense.weight', 'text_encoder_m.encoder.layer.0.intermediate.dense.bias', 'text_encoder_m.encoder.layer.0.output.dense.weight', 'text_encoder_m.encoder.layer.0.output.dense.bias', 'text_encoder_m.encoder.layer.0.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.0.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.attention.self.query.weight', 'text_encoder_m.encoder.layer.1.attention.self.query.bias', 'text_encoder_m.encoder.layer.1.attention.self.key.weight', 'text_encoder_m.encoder.layer.1.attention.self.key.bias', 'text_encoder_m.encoder.layer.1.attention.self.value.weight', 'text_encoder_m.encoder.layer.1.attention.self.value.bias', 'text_encoder_m.encoder.layer.1.attention.output.dense.weight', 'text_encoder_m.encoder.layer.1.attention.output.dense.bias', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.1.intermediate.dense.weight', 'text_encoder_m.encoder.layer.1.intermediate.dense.bias', 'text_encoder_m.encoder.layer.1.output.dense.weight', 'text_encoder_m.encoder.layer.1.output.dense.bias', 'text_encoder_m.encoder.layer.1.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.1.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.attention.self.query.weight', 'text_encoder_m.encoder.layer.2.attention.self.query.bias', 'text_encoder_m.encoder.layer.2.attention.self.key.weight', 'text_encoder_m.encoder.layer.2.attention.self.key.bias', 'text_encoder_m.encoder.layer.2.attention.self.value.weight', 'text_encoder_m.encoder.layer.2.attention.self.value.bias', 'text_encoder_m.encoder.layer.2.attention.output.dense.weight', 'text_encoder_m.encoder.layer.2.attention.output.dense.bias', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.2.intermediate.dense.weight', 'text_encoder_m.encoder.layer.2.intermediate.dense.bias', 'text_encoder_m.encoder.layer.2.output.dense.weight', 'text_encoder_m.encoder.layer.2.output.dense.bias', 'text_encoder_m.encoder.layer.2.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.2.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.attention.self.query.weight', 'text_encoder_m.encoder.layer.3.attention.self.query.bias', 'text_encoder_m.encoder.layer.3.attention.self.key.weight', 'text_encoder_m.encoder.layer.3.attention.self.key.bias', 'text_encoder_m.encoder.layer.3.attention.self.value.weight', 'text_encoder_m.encoder.layer.3.attention.self.value.bias', 'text_encoder_m.encoder.layer.3.attention.output.dense.weight', 'text_encoder_m.encoder.layer.3.attention.output.dense.bias', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.3.intermediate.dense.weight', 'text_encoder_m.encoder.layer.3.intermediate.dense.bias', 'text_encoder_m.encoder.layer.3.output.dense.weight', 'text_encoder_m.encoder.layer.3.output.dense.bias', 'text_encoder_m.encoder.layer.3.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.3.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.attention.self.query.weight', 'text_encoder_m.encoder.layer.4.attention.self.query.bias', 'text_encoder_m.encoder.layer.4.attention.self.key.weight', 'text_encoder_m.encoder.layer.4.attention.self.key.bias', 'text_encoder_m.encoder.layer.4.attention.self.value.weight', 'text_encoder_m.encoder.layer.4.attention.self.value.bias', 'text_encoder_m.encoder.layer.4.attention.output.dense.weight', 'text_encoder_m.encoder.layer.4.attention.output.dense.bias', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.4.intermediate.dense.weight', 'text_encoder_m.encoder.layer.4.intermediate.dense.bias', 'text_encoder_m.encoder.layer.4.output.dense.weight', 'text_encoder_m.encoder.layer.4.output.dense.bias', 'text_encoder_m.encoder.layer.4.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.4.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.attention.self.query.weight', 'text_encoder_m.encoder.layer.5.attention.self.query.bias', 'text_encoder_m.encoder.layer.5.attention.self.key.weight', 'text_encoder_m.encoder.layer.5.attention.self.key.bias', 'text_encoder_m.encoder.layer.5.attention.self.value.weight', 'text_encoder_m.encoder.layer.5.attention.self.value.bias', 'text_encoder_m.encoder.layer.5.attention.output.dense.weight', 'text_encoder_m.encoder.layer.5.attention.output.dense.bias', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.5.intermediate.dense.weight', 'text_encoder_m.encoder.layer.5.intermediate.dense.bias', 'text_encoder_m.encoder.layer.5.output.dense.weight', 'text_encoder_m.encoder.layer.5.output.dense.bias', 'text_encoder_m.encoder.layer.5.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.5.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.attention.self.query.weight', 'text_encoder_m.encoder.layer.6.attention.self.query.bias', 'text_encoder_m.encoder.layer.6.attention.self.key.weight', 'text_encoder_m.encoder.layer.6.attention.self.key.bias', 'text_encoder_m.encoder.layer.6.attention.self.value.weight', 'text_encoder_m.encoder.layer.6.attention.self.value.bias', 'text_encoder_m.encoder.layer.6.attention.output.dense.weight', 'text_encoder_m.encoder.layer.6.attention.output.dense.bias', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.6.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.6.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.6.intermediate.dense.weight', 'text_encoder_m.encoder.layer.6.intermediate.dense.bias', 'text_encoder_m.encoder.layer.6.output.dense.weight', 'text_encoder_m.encoder.layer.6.output.dense.bias', 'text_encoder_m.encoder.layer.6.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.6.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.attention.self.query.weight', 'text_encoder_m.encoder.layer.7.attention.self.query.bias', 'text_encoder_m.encoder.layer.7.attention.self.key.weight', 'text_encoder_m.encoder.layer.7.attention.self.key.bias', 'text_encoder_m.encoder.layer.7.attention.self.value.weight', 'text_encoder_m.encoder.layer.7.attention.self.value.bias', 'text_encoder_m.encoder.layer.7.attention.output.dense.weight', 'text_encoder_m.encoder.layer.7.attention.output.dense.bias', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.7.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.7.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.7.intermediate.dense.weight', 'text_encoder_m.encoder.layer.7.intermediate.dense.bias', 'text_encoder_m.encoder.layer.7.output.dense.weight', 'text_encoder_m.encoder.layer.7.output.dense.bias', 'text_encoder_m.encoder.layer.7.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.7.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.attention.self.query.weight', 'text_encoder_m.encoder.layer.8.attention.self.query.bias', 'text_encoder_m.encoder.layer.8.attention.self.key.weight', 'text_encoder_m.encoder.layer.8.attention.self.key.bias', 'text_encoder_m.encoder.layer.8.attention.self.value.weight', 'text_encoder_m.encoder.layer.8.attention.self.value.bias', 'text_encoder_m.encoder.layer.8.attention.output.dense.weight', 'text_encoder_m.encoder.layer.8.attention.output.dense.bias', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.8.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.8.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.8.intermediate.dense.weight', 'text_encoder_m.encoder.layer.8.intermediate.dense.bias', 'text_encoder_m.encoder.layer.8.output.dense.weight', 'text_encoder_m.encoder.layer.8.output.dense.bias', 'text_encoder_m.encoder.layer.8.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.8.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.attention.self.query.weight', 'text_encoder_m.encoder.layer.9.attention.self.query.bias', 'text_encoder_m.encoder.layer.9.attention.self.key.weight', 'text_encoder_m.encoder.layer.9.attention.self.key.bias', 'text_encoder_m.encoder.layer.9.attention.self.value.weight', 'text_encoder_m.encoder.layer.9.attention.self.value.bias', 'text_encoder_m.encoder.layer.9.attention.output.dense.weight', 'text_encoder_m.encoder.layer.9.attention.output.dense.bias', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.9.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.9.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.9.intermediate.dense.weight', 'text_encoder_m.encoder.layer.9.intermediate.dense.bias', 'text_encoder_m.encoder.layer.9.output.dense.weight', 'text_encoder_m.encoder.layer.9.output.dense.bias', 'text_encoder_m.encoder.layer.9.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.9.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.attention.self.query.weight', 'text_encoder_m.encoder.layer.10.attention.self.query.bias', 'text_encoder_m.encoder.layer.10.attention.self.key.weight', 'text_encoder_m.encoder.layer.10.attention.self.key.bias', 'text_encoder_m.encoder.layer.10.attention.self.value.weight', 'text_encoder_m.encoder.layer.10.attention.self.value.bias', 'text_encoder_m.encoder.layer.10.attention.output.dense.weight', 'text_encoder_m.encoder.layer.10.attention.output.dense.bias', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.10.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.10.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.10.intermediate.dense.weight', 'text_encoder_m.encoder.layer.10.intermediate.dense.bias', 'text_encoder_m.encoder.layer.10.output.dense.weight', 'text_encoder_m.encoder.layer.10.output.dense.bias', 'text_encoder_m.encoder.layer.10.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.10.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.attention.self.query.weight', 'text_encoder_m.encoder.layer.11.attention.self.query.bias', 'text_encoder_m.encoder.layer.11.attention.self.key.weight', 'text_encoder_m.encoder.layer.11.attention.self.key.bias', 'text_encoder_m.encoder.layer.11.attention.self.value.weight', 'text_encoder_m.encoder.layer.11.attention.self.value.bias', 'text_encoder_m.encoder.layer.11.attention.output.dense.weight', 'text_encoder_m.encoder.layer.11.attention.output.dense.bias', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.query.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.query.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.key.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.key.bias', 'text_encoder_m.encoder.layer.11.crossattention.self.value.weight', 'text_encoder_m.encoder.layer.11.crossattention.self.value.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder_m.encoder.layer.11.intermediate.dense.weight', 'text_encoder_m.encoder.layer.11.intermediate.dense.bias', 'text_encoder_m.encoder.layer.11.output.dense.weight', 'text_encoder_m.encoder.layer.11.output.dense.bias', 'text_encoder_m.encoder.layer.11.output.LayerNorm.weight', 'text_encoder_m.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.cls.predictions.bias', 'text_encoder.cls.predictions.transform.dense.weight', 'text_encoder.cls.predictions.transform.dense.bias', 'text_encoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.cls.predictions.transform.LayerNorm.bias', 'text_encoder.cls.predictions.decoder.weight', 'text_encoder.cls.predictions.decoder.bias'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/13 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c756f0cadfa463aa30c9d0c4b2a3b8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.855\n"
          ]
        }
      ],
      "source": [
        "albef_load_directory = TRAINED_MODELS_FOLDER + 'ALBEF'\n",
        "albef_model, albef_tokenizer, label_to_id, id_to_label = load_trained_models(albef_load_directory, image_model_type='albef')\n",
        "albef_predictions = predict(df_test.copy(), albef_model, albef_tokenizer, label_to_id, id_to_label, image_model_type='albef')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spu60NPV9rl9"
      },
      "source": [
        "### Save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwxOnHMn9rl-"
      },
      "outputs": [],
      "source": [
        "df_out = df_test.copy()\n",
        "df_out['bert_predictions'] = bert_predictions\n",
        "df_out['bert_resnet_predictions'] = bert_resnet_predictions\n",
        "df_out['albef_predictions'] = albef_predictions\n",
        "df_out.to_csv(RESULTS_FOLDER + 'predictions_with_pretrained_models.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mln4NsI-El0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b11716d184e94f73a155a1ca7af75b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_691f5be932334b2086d4f4b9cf762a01",
              "IPY_MODEL_862f014624354148a8fc31aca035619c",
              "IPY_MODEL_6626ccf6b4ec4eec82155cb9f99714fb"
            ],
            "layout": "IPY_MODEL_7eb05c94ebf44ed7b0498cf0e50eacff"
          }
        },
        "691f5be932334b2086d4f4b9cf762a01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_847e72b2a3494195975d51792cdf90ad",
            "placeholder": "​",
            "style": "IPY_MODEL_709baba96d894b80a8c395598e325a8a",
            "value": "100%"
          }
        },
        "862f014624354148a8fc31aca035619c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65965caa0baa4b5f811ffb2a55566c5f",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aaf2e15f2da64f8e93814544ec3bed4d",
            "value": 13
          }
        },
        "6626ccf6b4ec4eec82155cb9f99714fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4454b222760a4164a32583936154adfa",
            "placeholder": "​",
            "style": "IPY_MODEL_a0f50c19e79f42e2b3ac9fe117ca4c18",
            "value": " 13/13 [00:00&lt;00:00, 25.60it/s]"
          }
        },
        "7eb05c94ebf44ed7b0498cf0e50eacff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "847e72b2a3494195975d51792cdf90ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "709baba96d894b80a8c395598e325a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65965caa0baa4b5f811ffb2a55566c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaf2e15f2da64f8e93814544ec3bed4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4454b222760a4164a32583936154adfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0f50c19e79f42e2b3ac9fe117ca4c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "961aeaad14a24a00af0c38ac1862f052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_903cded981e84632b43d4c9953a89fd4",
              "IPY_MODEL_c836463c123e4b3489225d4c8d31135a",
              "IPY_MODEL_a0e1ace638974af195bf5ccb253342a3"
            ],
            "layout": "IPY_MODEL_85e85d1319d444b695d8f84464599818"
          }
        },
        "903cded981e84632b43d4c9953a89fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97c6d30f53884a47bdf2e0c5783c4632",
            "placeholder": "​",
            "style": "IPY_MODEL_8dceed58ac444454853fa9de178159fb",
            "value": "100%"
          }
        },
        "c836463c123e4b3489225d4c8d31135a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da5689c39b2e4da2b65b810ae3fc3d5a",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f180c827c4c4820a65a9bc2cc3be1ae",
            "value": 13
          }
        },
        "a0e1ace638974af195bf5ccb253342a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0684d9797c44e83a0d57e8ffbf0f26e",
            "placeholder": "​",
            "style": "IPY_MODEL_03e0663e4b764cee86304f43e12d94ff",
            "value": " 13/13 [00:00&lt;00:00, 21.07it/s]"
          }
        },
        "85e85d1319d444b695d8f84464599818": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97c6d30f53884a47bdf2e0c5783c4632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dceed58ac444454853fa9de178159fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da5689c39b2e4da2b65b810ae3fc3d5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f180c827c4c4820a65a9bc2cc3be1ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0684d9797c44e83a0d57e8ffbf0f26e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03e0663e4b764cee86304f43e12d94ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "258c3344438244bda943b63456e55bbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f888deb0a284dbc9b1bd7fac73a66b7",
              "IPY_MODEL_37ee1a4884934e4fbe636c6d0443622a",
              "IPY_MODEL_a1e0e47a1b9c4114a2f789b957f587c5"
            ],
            "layout": "IPY_MODEL_40057e4b0ac5408da574adfb9dd6f1f3"
          }
        },
        "9f888deb0a284dbc9b1bd7fac73a66b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c2061b5ac4a4196a5c0979b2db85a27",
            "placeholder": "​",
            "style": "IPY_MODEL_a031d36f9c89433b908f1654405225e9",
            "value": "100%"
          }
        },
        "37ee1a4884934e4fbe636c6d0443622a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c174102d5341435d92dafd964ee5a9e6",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b0940e48b4d4471923a578ad82c7514",
            "value": 13
          }
        },
        "a1e0e47a1b9c4114a2f789b957f587c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1cca46377c1475a9e294706e2aa6e99",
            "placeholder": "​",
            "style": "IPY_MODEL_dc95ef9b8dc54b00b940f4857dd88c85",
            "value": " 13/13 [00:01&lt;00:00,  6.98it/s]"
          }
        },
        "40057e4b0ac5408da574adfb9dd6f1f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c2061b5ac4a4196a5c0979b2db85a27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a031d36f9c89433b908f1654405225e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c174102d5341435d92dafd964ee5a9e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b0940e48b4d4471923a578ad82c7514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1cca46377c1475a9e294706e2aa6e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc95ef9b8dc54b00b940f4857dd88c85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c756f0cadfa463aa30c9d0c4b2a3b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_69677629740147e4b8f16a17117ba8ea",
              "IPY_MODEL_56de997b6020440cae66920886fbd51b",
              "IPY_MODEL_1910d86a931d4a5284cd9a7d9af2a666"
            ],
            "layout": "IPY_MODEL_e91818fa67aa479780d848e41246731f"
          }
        },
        "69677629740147e4b8f16a17117ba8ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7d1543a8826417da134ebf4e8f7c5cb",
            "placeholder": "​",
            "style": "IPY_MODEL_52edbad7017c4ad28487e9977294ec78",
            "value": "100%"
          }
        },
        "56de997b6020440cae66920886fbd51b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1136ba4fb24c0786f27febc8caa3d6",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ba9a244c50e4813a671502dc958245a",
            "value": 13
          }
        },
        "1910d86a931d4a5284cd9a7d9af2a666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07648340950f4463901de3d5c8cacff9",
            "placeholder": "​",
            "style": "IPY_MODEL_36095b727d7c4931ad234b1f32f2c900",
            "value": " 13/13 [00:03&lt;00:00,  3.86it/s]"
          }
        },
        "e91818fa67aa479780d848e41246731f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7d1543a8826417da134ebf4e8f7c5cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52edbad7017c4ad28487e9977294ec78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b1136ba4fb24c0786f27febc8caa3d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ba9a244c50e4813a671502dc958245a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07648340950f4463901de3d5c8cacff9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36095b727d7c4931ad234b1f32f2c900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de2d96325bc947e9a3fc1f587bfe3eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d60acccde21642129a3f1e9fe66c8784",
              "IPY_MODEL_7f63c2f548f0478d88d6c2bffcffa1a2",
              "IPY_MODEL_3f3562fa5c0a4e0ea4f8be5b49ed8e5d"
            ],
            "layout": "IPY_MODEL_f811840e755e424dba04de4a081414a8"
          }
        },
        "d60acccde21642129a3f1e9fe66c8784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f902f3c8ed74a868be7f50b5b25733e",
            "placeholder": "​",
            "style": "IPY_MODEL_05166af3c5af409d84a59792991eb8fc",
            "value": "model.safetensors: 100%"
          }
        },
        "7f63c2f548f0478d88d6c2bffcffa1a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6edb9af6e494b3fae39b257c53d301b",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0f41b6fbea640ebb85c9709c39a90e5",
            "value": 440449768
          }
        },
        "3f3562fa5c0a4e0ea4f8be5b49ed8e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_777fd002e1dc497493cbcabacb22d979",
            "placeholder": "​",
            "style": "IPY_MODEL_53d331f802a1429ba2b7a0ae927ee061",
            "value": " 440M/440M [00:06&lt;00:00, 92.1MB/s]"
          }
        },
        "f811840e755e424dba04de4a081414a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f902f3c8ed74a868be7f50b5b25733e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05166af3c5af409d84a59792991eb8fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e6edb9af6e494b3fae39b257c53d301b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0f41b6fbea640ebb85c9709c39a90e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "777fd002e1dc497493cbcabacb22d979": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53d331f802a1429ba2b7a0ae927ee061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}